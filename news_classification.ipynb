{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "from fastai.text import *\n",
    "from fastai import *\n",
    "from fastai.callbacks import *\n",
    "from transformers import AdamW\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
    "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
    "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
    "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
    "    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
    "    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n",
    "    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "seed = 42\n",
    "use_fp16 = False\n",
    "bs = 16\n",
    "\n",
    "model_type = 'bert'\n",
    "pretrained_model_name = 'bert-base-multilingual-cased'\n",
    "pretrained_model_path = 'model/bert4vn/pytorch_model.bin'\n",
    "\n",
    "# model_type = 'bert'\n",
    "# pretrained_model_name='bert-base-uncased'\n",
    "\n",
    "# model_type = 'distilbert'\n",
    "# pretrained_model_name = 'distilbert-base-uncased'\n",
    "\n",
    "#model_type = 'xlm'\n",
    "#pretrained_model_name = 'xlm-clm-enfr-1024'\n",
    "\n",
    "# model_type = 'xlnet'\n",
    "# pretrained_model_name = 'xlnet-base-cased'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "path = Path('.')\n",
    "train = pd.read_csv(path/'medical_data/title/train.csv', sep='\\t')\n",
    "test = pd.read_csv(path/'medical_data/title/test.csv', sep='\\t')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "seed_all(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class TransformersBaseTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper PretrainedTokenizer with tokenizer of fastai\"\"\"\n",
    "    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type='bert'):\n",
    "        super(TransformersBaseTokenizer, self).__init__(lang='')\n",
    "        self._pretrained_tokenizer = pretrained_tokenizer\n",
    "        self.max_seq_len = pretrained_tokenizer.max_len\n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        \"\"\"Limits the maximum sequence length and add special tokens\"\"\"\n",
    "        CLS = self._pretrained_tokenizer.cls_token\n",
    "        SEP = self._pretrained_tokenizer.sep_token\n",
    "        \n",
    "        if self.model_type in ['roberta']:\n",
    "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n",
    "            tokens = [CLS] + tokens + [SEP]\n",
    "        else:\n",
    "            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n",
    "            if self.model_type in ['xlnet']:\n",
    "                tokens = tokens + [SEP] + [CLS]\n",
    "            else:\n",
    "                tokens = [CLS] + tokens + [SEP]\n",
    "        return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
    "transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer=transformer_tokenizer, model_type=model_type)\n",
    "fastai_tokenizer = Tokenizer(tok_func=transformer_base_tokenizer, pre_rules=[], post_rules=[])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class TransformersVocab(Vocab):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "        super(TransformersVocab, self).__init__(itos=[])\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
    "        \"\"\"Convert a list of tokens 't' to their ids\"\"\"\n",
    "        return self.tokenizer.convert_tokens_to_ids(t)\n",
    "    \n",
    "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
    "        \"\"\"Convert a list of numbers to their tokens\"\"\"\n",
    "        nums = np.array(nums).tolist()\n",
    "        return (\n",
    "            sep.join(self.tokenizer.convert_ids_to_tokens(nums)) \n",
    "            if sep is not None \n",
    "            else self.tokenizer.convert_ids_to_tokens(nums)\n",
    "        )\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        return {'itos': self.itos, 'tokenizer': self.tokenizer}\n",
    "    \n",
    "    def __setstate__(self, state: dict):\n",
    "        self.itos = state['itos']\n",
    "        self.tokenizer = state['tokenizer']\n",
    "        self.stoi = collections.defaultdict(int, {v: k for k, v in enumerate(self.itos)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "transformer_vocab = TransformersVocab(tokenizer=transformer_tokenizer)\n",
    "numericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n",
    "\n",
    "tokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, \n",
    "                                       include_bos=False, \n",
    "                                       include_eos=False)\n",
    "\n",
    "transformer_processor = [tokenize_processor, numericalize_processor]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "pad_first = bool(model_type in ['xlnet'])\n",
    "pad_idx = transformer_tokenizer.pad_token_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='15', style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='2', style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='7', style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "databunch = (TextList.from_df(train, cols='text', processor=transformer_processor)\n",
    "             .split_by_rand_pct(0.1, seed=seed)\n",
    "             .label_from_df(cols='label')\n",
    "             .add_test(test)\n",
    "             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[CLS] token:  <s>\n[SEP] token:  </s>\n[PAD] token:  <pad>\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>&lt;s&gt; Ġ- L RB - ĠCity Ġ- RR B - Ġreminds Ġus Ġhow Ġrealistically Ġnuanced Ġa ĠRobert ĠDe ĠN iro Ġperformance Ġcan Ġbe Ġwhen Ġhe Ġis Ġnot Ġmore Ġluc r atively Ġengaged Ġin Ġthe Ġshameless Ġself - car ic ature Ġof Ġ` ĠAnaly ze ĠThis Ġ' Ġ- L RB - Ġ1999 Ġ- RR B - Ġand Ġ` ĠAnaly ze ĠThat Ġ, Ġ' Ġpromised Ġ- L RB - Ġor Ġthreatened Ġ-</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>&lt;s&gt; ĠThe Ġreal Ġtriumph s Ġin ĠIg by Ġcome Ġfrom ĠPhilippe Ġ, Ġwho Ġmakes ĠOliver Ġfar Ġmore Ġinteresting Ġthan Ġthe Ġcharacter Ġ' s Ġlines Ġwould Ġsuggest Ġ, Ġand ĠSar andon Ġ, Ġwho Ġcould Ġn 't Ġbe Ġbetter Ġas Ġa Ġcruel Ġbut Ġweird ly Ġlik able ĠWAS P Ġmat ron Ġ. &lt;/s&gt;</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>&lt;s&gt; ĠParker Ġshould Ġbe Ġcomm ended Ġfor Ġtaking Ġa Ġfresh Ġapproach Ġto Ġfamiliar Ġmaterial Ġ, Ġbut Ġhis Ġdetermination Ġto Ġremain Ġtrue Ġto Ġthe Ġoriginal Ġtext Ġleads Ġhim Ġto Ġadopt Ġa Ġsomewhat Ġman nered Ġtone Ġ... Ġthat Ġultimately Ġdull s Ġthe Ġhuman Ġtragedy Ġat Ġthe Ġstory Ġ' s Ġcore &lt;/s&gt;</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>&lt;s&gt; ĠIt Ġ' s Ġa Ġlong Ġway Ġfrom ĠOrwell Ġ' s Ġdark Ġ, Ġintelligent Ġwarning Ġcry Ġ- L RB - Ġ1984 Ġ- RR B - Ġto Ġthe Ġempty Ġstud Ġknock about Ġof ĠEqu ilibrium Ġ, Ġand Ġwhat Ġonce Ġwas Ġconviction Ġis Ġnow Ġaffect ation Ġ. &lt;/s&gt;</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>&lt;s&gt; ĠA Ġdifferent Ġand Ġemotionally Ġreserved Ġtype Ġof Ġsurvival Ġstory Ġ-- Ġa Ġfilm Ġless Ġabout Ġref ract ing Ġall Ġof ĠWorld ĠWar ĠII Ġthrough Ġthe Ġspecific Ġconditions Ġof Ġone Ġman Ġ, Ġand Ġmore Ġabout Ġthat Ġman Ġlost Ġin Ġits Ġmidst Ġ. &lt;/s&gt;</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"[CLS] token: \", transformer_tokenizer.cls_token)\n",
    "print(\"[SEP] token: \", transformer_tokenizer.sep_token)\n",
    "print(\"[PAD] token: \", transformer_tokenizer.pad_token)\n",
    "databunch.show_batch()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[CLS] id:  0\n[SEP] id:  2\n[PAD] id:  1\n",
      "Batch shape:  torch.Size([16, 79])\ntensor([[    0,   111,   574,  ...,    76,   479,     2],\n        [    0,    33,     7,  ...,     1,     1,     1],\n        [    0,   318,    47,  ...,     1,     1,     1],\n        ...,\n        [    0,     5,  2156,  ...,     1,     1,     1],\n        [    0,    33, 30291,  ...,     1,     1,     1],\n        [    0, 45518, 10730,  ...,     1,     1,     1]])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"[CLS] id: \", transformer_tokenizer.cls_token_id)\n",
    "print(\"[SEP] id: \", transformer_tokenizer.sep_token_id)\n",
    "print(\"[PAD] id: \", pad_idx)\n",
    "test_one_batch = databunch.one_batch()[0]\n",
    "print('Batch shape: ', test_one_batch.shape)\n",
    "print(test_one_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, transformer_model: PreTrainedModel):\n",
    "        super(CustomTransformerModel, self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        attention_mask = (input_ids != pad_idx).type(input_ids.type())\n",
    "        logits = self.transformer(input_ids, attention_mask=attention_mask)[0]\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "RobertaConfig {\n  \"_num_labels\": 5,\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"do_sample\": false,\n  \"early_stopping\": false,\n  \"eos_token_id\": 2,\n  \"finetuning_task\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"is_decoder\": false,\n  \"is_encoder_decoder\": false,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"length_penalty\": 1.0,\n  \"max_length\": 20,\n  \"max_position_embeddings\": 514,\n  \"min_length\": 0,\n  \"model_type\": \"roberta\",\n  \"no_repeat_ngram_size\": 0,\n  \"num_attention_heads\": 12,\n  \"num_beams\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_return_sequences\": 1,\n  \"output_attentions\": false,\n  \"output_hidden_states\": false,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"pruned_heads\": {},\n  \"repetition_penalty\": 1.0,\n  \"temperature\": 1.0,\n  \"top_k\": 50,\n  \"top_p\": 1.0,\n  \"torchscript\": false,\n  \"type_vocab_size\": 1,\n  \"use_bfloat16\": false,\n  \"vocab_size\": 50265\n}\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "config = config_class.from_pretrained(pretrained_model_name)\n",
    "config.num_labels = 5\n",
    "config.use_bfloat16 = use_fp16\n",
    "print(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "transfromer_model = model_class.from_pretrained(pretrained_model_name_or_path=path/'model/roberta-base/pytorch_model.bin', config=config)\n",
    "custom_transformer_model = CustomTransformerModel(transformer_model=transfromer_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "CustomAdamW = partial(AdamW, correct_bias=False)\n",
    "\n",
    "learner = Learner(\n",
    "    databunch,\n",
    "    custom_transformer_model,\n",
    "    opt_func=CustomAdamW,\n",
    "    metrics=[accuracy, error_rate]\n",
    ")\n",
    "\n",
    "learner.callbacks.append(ShowGraph(learner))\n",
    "\n",
    "if use_fp16:\n",
    "    learner = learner.to_fp16()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "CustomTransformerModel(\n  (transformer): RobertaForSequenceClassification(\n    (roberta): RobertaModel(\n      (embeddings): RobertaEmbeddings(\n        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n        (position_embeddings): Embedding(514, 768, padding_idx=1)\n        (token_type_embeddings): Embedding(1, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (6): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (7): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (8): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (9): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (10): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (11): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (classifier): RobertaClassificationHead(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (out_proj): Linear(in_features=768, out_features=5, bias=True)\n    )\n  )\n)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(learner.model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "list_layers = [\n",
    "    learner.model.transformer.roberta.embeddings,\n",
    "    learner.model.transformer.roberta.encoder.layer[0],\n",
    "    learner.model.transformer.roberta.encoder.layer[1],\n",
    "    learner.model.transformer.roberta.encoder.layer[2],\n",
    "    learner.model.transformer.roberta.encoder.layer[3],\n",
    "    learner.model.transformer.roberta.encoder.layer[4],\n",
    "    learner.model.transformer.roberta.encoder.layer[5],\n",
    "    learner.model.transformer.roberta.encoder.layer[6],\n",
    "    learner.model.transformer.roberta.encoder.layer[7],\n",
    "    learner.model.transformer.roberta.encoder.layer[8],\n",
    "    learner.model.transformer.roberta.encoder.layer[9],\n",
    "    learner.model.transformer.roberta.encoder.layer[10],\n",
    "    learner.model.transformer.roberta.encoder.layer[11],\n",
    "    learner.model.transformer.roberta.pooler,\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Learner split in 14  groups\n[Sequential(\n  (0): Embedding(50265, 768, padding_idx=1)\n  (1): Embedding(514, 768, padding_idx=1)\n  (2): Embedding(1, 768)\n  (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (4): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Tanh()\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=5, bias=True)\n)]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "learner.split(list_layers)\n",
    "num_groups = len(learner.layer_groups)\n",
    "print('Learner split in', num_groups, ' groups')\n",
    "print(learner.layer_groups)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "learner.save('untrain')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "Learner(data=TextClasDataBunch;\n\nTrain: LabelList (140454 items)\nx: TextList\n<s> ĠA Ġseries Ġof Ġesc ap ades Ġdemonstrating Ġthe Ġad age Ġthat Ġwhat Ġis Ġgood Ġfor Ġthe Ġgoose </s>,<s> ĠA Ġseries </s>,<s> ĠA </s>,<s> Ġseries </s>,<s> Ġof Ġesc ap ades Ġdemonstrating Ġthe Ġad age Ġthat Ġwhat Ġis Ġgood Ġfor Ġthe Ġgoose </s>\ny: CategoryList\n2,2,2,2,2\nPath: .;\n\nValid: LabelList (15606 items)\nx: TextList\n<s> Ġ' s Ġas Ġsorry </s>,<s> ĠRomantic Ġcomedy Ġand ĠDog me Ġ95 Ġfilmmaking Ġmay Ġseem Ġodd Ġbed fell ows Ġ, Ġbut Ġthey Ġturn Ġout Ġto Ġbe Ġdelight fully Ġcompatible Ġhere </s>,<s> Ġof Ġthese Ġdays </s>,<s> Ġfl inch Ġfrom Ġits Ġunsettling Ġprog nosis </s>,<s> Ġare Ġclinically Ġdepressed </s>\ny: CategoryList\n2,4,2,2,1\nPath: .;\n\nTest: LabelList (66292 items)\nx: TextList\n<s> Ġ15 60 61 Ġ85 45 ĠAn Ġintermitt ently Ġpleasing Ġbut Ġmostly Ġroutine Ġeffort Ġ. </s>,<s> Ġ15 60 62 Ġ85 45 ĠAn Ġintermitt ently Ġpleasing Ġbut Ġmostly Ġroutine Ġeffort </s>,<s> Ġ15 60 63 Ġ85 45 ĠAn </s>,<s> Ġ15 60 64 Ġ85 45 Ġintermitt ently Ġpleasing Ġbut Ġmostly Ġroutine Ġeffort </s>,<s> Ġ15 60 65 Ġ85 45 Ġintermitt ently Ġpleasing Ġbut Ġmostly Ġroutine </s>\ny: EmptyLabelList\n,,,,\nPath: ., model=CustomTransformerModel(\n  (transformer): RobertaForSequenceClassification(\n    (roberta): RobertaModel(\n      (embeddings): RobertaEmbeddings(\n        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n        (position_embeddings): Embedding(514, 768, padding_idx=1)\n        (token_type_embeddings): Embedding(1, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (6): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (7): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (8): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (9): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (10): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (11): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (classifier): RobertaClassificationHead(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (out_proj): Linear(in_features=768, out_features=5, bias=True)\n    )\n  )\n), opt_func=functools.partial(<class 'transformers.optimization.AdamW'>, correct_bias=False), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f3e528f81e0>, <function error_rate at 0x7f3e528f8400>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[ShowGraph\nlearn: Learner(data=TextClasDataBunch;\n\nTrain: LabelList (140454 items)\nx: TextList\n<s> ĠA Ġseries Ġof Ġesc ap ades Ġdemonstrating Ġthe Ġad age Ġthat Ġwhat Ġis Ġgood Ġfor Ġthe Ġgoose </s>,<s> ĠA Ġseries </s>,<s> ĠA </s>,<s> Ġseries </s>,<s> Ġof Ġesc ap ades Ġdemonstrating Ġthe Ġad age Ġthat Ġwhat Ġis Ġgood Ġfor Ġthe Ġgoose </s>\ny: CategoryList\n2,2,2,2,2\nPath: .;\n\nValid: LabelList (15606 items)\nx: TextList\n<s> Ġ' s Ġas Ġsorry </s>,<s> ĠRomantic Ġcomedy Ġand ĠDog me Ġ95 Ġfilmmaking Ġmay Ġseem Ġodd Ġbed fell ows Ġ, Ġbut Ġthey Ġturn Ġout Ġto Ġbe Ġdelight fully Ġcompatible Ġhere </s>,<s> Ġof Ġthese Ġdays </s>,<s> Ġfl inch Ġfrom Ġits Ġunsettling Ġprog nosis </s>,<s> Ġare Ġclinically Ġdepressed </s>\ny: CategoryList\n2,4,2,2,1\nPath: .;\n\nTest: LabelList (66292 items)\nx: TextList\n<s> Ġ15 60 61 Ġ85 45 ĠAn Ġintermitt ently Ġpleasing Ġbut Ġmostly Ġroutine Ġeffort Ġ. </s>,<s> Ġ15 60 62 Ġ85 45 ĠAn Ġintermitt ently Ġpleasing Ġbut Ġmostly Ġroutine Ġeffort </s>,<s> Ġ15 60 63 Ġ85 45 ĠAn </s>,<s> Ġ15 60 64 Ġ85 45 Ġintermitt ently Ġpleasing Ġbut Ġmostly Ġroutine Ġeffort </s>,<s> Ġ15 60 65 Ġ85 45 Ġintermitt ently Ġpleasing Ġbut Ġmostly Ġroutine </s>\ny: EmptyLabelList\n,,,,\nPath: ., model=CustomTransformerModel(\n  (transformer): RobertaForSequenceClassification(\n    (roberta): RobertaModel(\n      (embeddings): RobertaEmbeddings(\n        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n        (position_embeddings): Embedding(514, 768, padding_idx=1)\n        (token_type_embeddings): Embedding(1, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (6): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (7): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (8): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (9): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (10): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (11): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (classifier): RobertaClassificationHead(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (out_proj): Linear(in_features=768, out_features=5, bias=True)\n    )\n  )\n), opt_func=functools.partial(<class 'transformers.optimization.AdamW'>, correct_bias=False), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f3e528f81e0>, <function error_rate at 0x7f3e528f8400>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n  (0): Embedding(50265, 768, padding_idx=1)\n  (1): Embedding(514, 768, padding_idx=1)\n  (2): Embedding(1, 768)\n  (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (4): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Tanh()\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=5, bias=True)\n)], add_time=True, silent=False)], layer_groups=[Sequential(\n  (0): Embedding(50265, 768, padding_idx=1)\n  (1): Embedding(514, 768, padding_idx=1)\n  (2): Embedding(1, 768)\n  (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (4): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Linear(in_features=768, out_features=768, bias=True)\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=768, bias=True)\n  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (6): Dropout(p=0.1, inplace=False)\n  (7): Linear(in_features=768, out_features=3072, bias=True)\n  (8): Linear(in_features=3072, out_features=768, bias=True)\n  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (10): Dropout(p=0.1, inplace=False)\n), Sequential(\n  (0): Linear(in_features=768, out_features=768, bias=True)\n  (1): Tanh()\n  (2): Linear(in_features=768, out_features=768, bias=True)\n  (3): Dropout(p=0.1, inplace=False)\n  (4): Linear(in_features=768, out_features=5, bias=True)\n)], add_time=True, silent=False)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 26
    }
   ],
   "source": [
    "seed_all(seed)\n",
    "learner.load('untrain')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "learner.freeze_to(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "CustomTransformerModel\n======================================================================\nLayer (type)         Output Shape         Param #    Trainable \n======================================================================\nEmbedding            [79, 768]            38,603,520 False     \n______________________________________________________________________\nEmbedding            [79, 768]            394,752    False     \n______________________________________________________________________\nEmbedding            [79, 768]            768        False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nDropout              [12, 79, 79]         0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 3072]           2,362,368  False     \n______________________________________________________________________\nLinear               [79, 768]            2,360,064  False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nDropout              [12, 79, 79]         0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 3072]           2,362,368  False     \n______________________________________________________________________\nLinear               [79, 768]            2,360,064  False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nDropout              [12, 79, 79]         0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 3072]           2,362,368  False     \n______________________________________________________________________\nLinear               [79, 768]            2,360,064  False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nDropout              [12, 79, 79]         0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 3072]           2,362,368  False     \n______________________________________________________________________\nLinear               [79, 768]            2,360,064  False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nDropout              [12, 79, 79]         0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 3072]           2,362,368  False     \n______________________________________________________________________\nLinear               [79, 768]            2,360,064  False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nDropout              [12, 79, 79]         0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 3072]           2,362,368  False     \n______________________________________________________________________\nLinear               [79, 768]            2,360,064  False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nDropout              [12, 79, 79]         0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 3072]           2,362,368  False     \n______________________________________________________________________\nLinear               [79, 768]            2,360,064  False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nDropout              [12, 79, 79]         0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 3072]           2,362,368  False     \n______________________________________________________________________\nLinear               [79, 768]            2,360,064  False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nDropout              [12, 79, 79]         0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 3072]           2,362,368  False     \n______________________________________________________________________\nLinear               [79, 768]            2,360,064  False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nDropout              [12, 79, 79]         0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 3072]           2,362,368  False     \n______________________________________________________________________\nLinear               [79, 768]            2,360,064  False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nDropout              [12, 79, 79]         0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 3072]           2,362,368  False     \n______________________________________________________________________\nLinear               [79, 768]            2,360,064  False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nDropout              [12, 79, 79]         0          False     \n______________________________________________________________________\nLinear               [79, 768]            590,592    False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [79, 3072]           2,362,368  False     \n______________________________________________________________________\nLinear               [79, 768]            2,360,064  False     \n______________________________________________________________________\nLayerNorm            [79, 768]            1,536      False     \n______________________________________________________________________\nDropout              [79, 768]            0          False     \n______________________________________________________________________\nLinear               [768]                590,592    True      \n______________________________________________________________________\nTanh                 [768]                0          False     \n______________________________________________________________________\nLinear               [768]                590,592    True      \n______________________________________________________________________\nDropout              [768]                0          False     \n______________________________________________________________________\nLinear               [5]                  3,845      True      \n______________________________________________________________________\n\nTotal params: 125,240,069\nTotal trainable params: 1,185,029\nTotal non-trainable params: 124,055,040\nOptimized with 'transformers.optimization.AdamW', correct_bias=False\nUsing true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \nLoss function : FlattenedLoss\n======================================================================\nCallbacks functions applied \n    ShowGraph"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 28
    }
   ],
   "source": [
    "learner.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    \n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "learner.lr_find()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Min numerical gradient: 9.12E-05\nMin loss divided by 10: 1.91E-05\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEGCAYAAABLgMOSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXicZbn48e89k73ZmrVZuiXpvtN0Yy2rBZSCooKiBxUQQTyiHEX5HfXgUTluKPQcEVkVBBSxrJUdytKFdEla2kLbpEvS7MtkX2bm+f0xkzRts3Uyk3dmen+ua65O3vXO22TuPLsYY1BKKaVOls3qAJRSSoUmTSBKKaV8oglEKaWUTzSBKKWU8okmEKWUUj6JsDoAf0pLSzNTpkyxOgyllAoZW7ZsqTPGpPtyblglkClTplBUVGR1GEopFTJE5KCv52oVllJKKZ9oAlFKKeWTgCUQEXlIRGpEZOcg+1eKiENEtntfP+q371YR+VBEdorIEyISE6g4lVJK+SaQJZBHgFXDHPOOMWah93UngIjkAN8CCo0xcwE7cFUA41RKKeWDgCUQY8x6oMHH0yOAWBGJAOKAI34LTCmllF9Y3QayQkSKRWSdiMwBMMZUAL8GDgGVgMMY88pgFxCRG0SkSESKamtrxyZqpZRSliaQrcBkY8wC4F5gLYCIjAdWA1OBbGCciFwz2EWMMfcbYwqNMYXp6T51ZVZKKeUDyxKIMabZGNPqff8SECkiacAFQJkxptYY0wM8A5xuVZyjYYyhs8dFfWsX3U73mN53Z4WDB94pZXNZA06X/+9d29LFCyVHcHT0+P3aSqnQYNlAQhGZAFQbY4yILMWTzOrxVF0tF5E4oAM4Hwia0YHdTjc/fWEXZXVtuI3B5TYYg+e9MXR0u2jpdNLa5aSty4nT7VlvRQRSx0UzISmaCYkxZCbGkJEQg02gx+Wm22Vwutz0uNwYYFpmAosnjWfGhATsNhlRbNXNnTy7vYJ/bKngo+qWvu3j4yI5d2YGF87K5Kzp6cRH+/7f7nYb/lZ0mJ+/tJvmTidxUXauXJzLtadPIS893ufrKqVCT8ASiIg8AawE0kSkHPgxEAlgjLkPuBL4hog48SSKq4xndatNIvI0niouJ7ANuD9QcQLsKHcwJS2OhJjIYY+95/W9/GXjQRZOTCbSLthEsNuESJvnfXp8NPExEcRHe18xEcRG2mlq76G6uZOq5k7KGzvYcrCRxvajf71H2W1E2IVIuw23MbR0OgGIj45g0aRkTps0noUTk4mOtOF2g8sYXG43Ljc0tnfzQkkl7+6txW1g0aRkfnr5XM6dkU5JuYPXdlXzxp4antlaQZTdxpnT0vjp5XPJSY49qee0r6aVH/5zB5vLGlg6NYVvnJPPizsqeXLzYf684SDnzczgq2dM5YyCVERGlvSUUqFLwmlFwsLCQnOyU5k0tnVz5v+8wZKpKTzw5UIi7IPX6m052Mhn73ufT5+Wy68/u2C04dLjciOA3SbHfOAaY/qSTO9rT1Uz7iH+q3KSY/n0aTlcsShnwJKA0+Vmy8FGXttdzZMfHCYxJpLHrlvG1LRxw8bZ5XRx31ul/O+b+4iJtHHHpbP47OKJ2Lwlo9qWLh7fdJDHNh6krrWb2VmJPPyVJWQm6vAdpYKdiGwxxhT6dO6pnkAA/rrpED/85w6uPX0KP7lszoDHtHU5ueSed3C6DP/69lkjKq34U2uXk92VzThdBrtNsNvAbrNhFyE60kZBenzfB/pwPjzi4MsPbkZEeOy6pcyckDjosZtK67lj7U721bRy2YJs/vOTs0lPiB7w2C6ni+e2H+HHz33I5NRx/O3ry8f8OSmlTo4mEC9fEwjAT1/YxYPvlvHT1XP40oopJ+z/wTM7ePKDQzx5/XKW5aWOMlLr7atp5ZoHNtHpdPHoV5ayYGLyMftrW7r4xbrdPLO1gpzkWP778rmcOzNjRNd+66Marnu0iGV5KTx87VKiIqzuLa6UGsxoEoj+Znv98JJZnD8zg588v4v1Hx87nuSNPdU8sfkQN5yVFxbJA6AgI56/37iChJgIvvjAJjaV1gPgchv+svEg5//mLZ4vPsLN5+bz2nfOGXHyAFg5I4NffHoe7+2r53tPF+Mequ5NKRWytATST2uXkyv/8D4VjR08c9PpTMtMoL61i0/87h3S4qN49ptnEB1h92PE1qtydHLNg5sob2znh5fM4ukt5ZSUOzg9P5U7V8+lIMP3nlVr3tjLr1/5mBvPyef2i2f6MWqllL9oCcRP4qMjePDaJURH2vnqox9Q39rFD/+5g+aOHu7+/MKwSx4AE5JieOqG5eSnx/OjZz+k0tHJPVcv4vHrlo0qeQDcfG4BX1g2ifve3s+j7x/wT8BKqaChJZABbDvUyFX3byQtPpqKpg5+cPFMvn5Ovh8iDF7NnT2s21HJxfOySPRjw7fT5ebGx7by+p5q/vDF01g1N8tv11ZKjZ6WQPxs0aTx/OZzC6ho6mDplBSuOyvP6pACLjEmks8vmeTX5AEQYbdx79WLWDgxmVue2MavX/6Izh6XX++hlLKGlkCGsOVgIwXp8STFaVfU0Wpq7+bO53fxzLYKJqXEcefqOaycMfKGeaVUYGgJJEAWTx6vycNPkuOi+O3nF/LX65cRYReuffgDbn58K1WOTqtDU0r5SBOIGlOn56ex7t/P4raLpvPa7mou+O3bPPr+AcKpJKzUqUITiBpz0RF2vnneNF659WxOmzyeHz/3Id9+aru2jSgVYjSBKMtMTh3Ho19ZwvdWzeC54iN8/o8bqG7WKi2lTsbhhnYqmjosKcVrAlGWEhFuWlnAH69ZzN6aVi5b8y4l5U1Wh6VUyPjD2/tZ9bv1ltxbE4gKChfNmcAzN51OpN3GZ+/bwHPFR6wOSamQUFLexPzcJEuWUNAEooLGzAmJPHvzGSzITeZbT2zj7lc/tjokpYJaZ4+LPZUtzM9NHv7gANAEooJKanw0j123jM+clsvvX9/L67urrQ5JqaC1p6oFp9uwIDfJkvtrAlFBJyrCxi8+PY/pmfH859qdtHU5rQ5JqaDU2144T0sgSh3lSSLzqWzu5DevaFWWUgMpPuwgLT6K7CRrVv/UBKKC1uLJ47lm2WQeeb+M4sPaM0up4+2oaGJ+brIlDeigCUQFuf9YNYP0hGhuf2YHPS631eEoFTTaupzsq2llXo417R+gCUQFucSYSP7rsrnsrmzmwXfLrA5HqaCxs8KB28CCiZpAlBrUqrkTuGh2Jr977WMO1rdZHY5SQWFHhQOAeTnWNKCDJhAVIu5cPZcIm43/t3anTryoFFBc7iA7KYb0hGjLYtAEokLChKQYvr9qBu/srWPt9gqrw1HKcp4R6NaVPiCACUREHhKRGhHZOcj+lSLiEJHt3teP+u1LFpGnRWSPiOwWkRWBilOFji8um8xpk5K54587+dP6Urqd2qiuTk2O9h4O1rcz38L2DwhsCeQRYNUwx7xjjFnofd3Zb/vvgX8ZY2YCC4DdAYpRhRCbTfjfL57G8rxUfvbSbi7+/XrWf1xrdVhKjbmSCk+39vkWtn9AABOIMWY90HCy54lIInA28KD3Ot3GGB0EoADISorloWuX8NC1hbjchi8/tJnr/1zEofp2q0NTasyUlHsb0C2awqSX1W0gK0SkWETWicgc77Y8oBZ4WES2icgDIjJusAuIyA0iUiQiRbW1+tfoqeK8mZm8fOvZfH/VTN7bV8cFd7/NH9/eb3VYSo2JkvImpqaNIynW2iW3rUwgW4HJxpgFwL3AWu/2COA04A/GmEVAG3D7YBcxxtxvjCk0xhSmp6cHOmYVRKIj7HxjZT5v3raSM/JTuetfe6h0dFgdllIBV1LusHQAYS/LEogxptkY0+p9/xIQKSJpQDlQbozZ5D30aTwJRakBZSbG8ONPzcEYeG67riOiwltNSyeVjk7mW1x9BRYmEBGZIN4JXERkqTeWemNMFXBYRGZ4Dz0f2GVRmCpETEkbx6JJyfxzm3bxVeFth7f9w+ouvBDYbrxPABuAGSJSLiJfE5EbReRG7yFXAjtFpBi4B7jKHB0hdgvwuIiUAAuBnwcqThU+rliUw56qFnZXNlsdilIBU1zuwCYwNyfR6lCICNSFjTFXD7N/DbBmkH3bgcJAxKXC16Xzsrjz+V2s3V7BrCzrf7mUCoSS8iamZSQQFxWwj+8Rs7oXllJ+kxofzTnT03l22xHcbp3uRIUfYww7yh2Wd9/tpQlEhZXLF+VQ1dzJxtJ6q0NRyu8qmjqob+u2bAnb42kCUWHlwtmZxEdHaGO6CkslQdSADppAVJiJibRz8dwJrNtZRWePy+pwlPKrknIHkXZhZlaC1aEAmkBUGLpiUQ6tXU5e211tdShK+VVJeRMzJyQSHWG3OhRAE4gKQ8vyUpmQGMNarcZSYcTt9jSgB8MAwl6aQFTYsduE1QuzeeujWhrauq0ORym/OFDfRkuXUxOIUoF2+aIcnG7DiyU6tYkKD8HWgA6aQFSYmpWVyMwJCdobS4WNXZXNREXYmJYRb3UofTSBqLB1+aIcth5q4mB9m9WhKDVq9a3dpMdHE2EPno/t4IlEKT+7bEE2IrB2m1ZjqdDn6Ogm0eL1P46nCUSFrezkWJZPTeWJzYc40qTrhKjQ5ujoISnW+vmv+tMEosLa7RfPpK3Lyef+uIHDDbrsrQpdjo4ekmOjrA7jGJpAVFhbMDGZx69fRkunJ4mU1Wl7iApNnhKIVmEpNabm5ybzxPXL6XK6+dwfN7C3usXqkJQ6aY6OHpLiNIEoNeZmZyfy1A3LAbjq/o266JQKKZ09Ljp73FoCUcoq0zITeOqG5URF2Lj6Txv7lgZVKtg1d/QAaC8spayUlx7P376+gvjoCL7+lyJdeEqFBIc3gSRrAlHKWhNT4vjeqpkccXSy+UCD1eEoNazeBKJVWEoFgQtmZRAbaee5Yh1kqIJfU7smEKWCRlxUBBfOzmTdjkp6XG6rw1FqSFoCUSrIXLYgm8b2Ht7dW2d1KEoNqa8NRLvxKhUczp6eTmJMhFZjqaDXm0ASYjSBKBUUoiJsXDw3i1c+rKKjW9dPV8HL0dFDQkwEdptYHcoxApZAROQhEakRkZ2D7F8pIg4R2e59/ei4/XYR2SYiLwQqRqUuW5hNW7eLN/bUWB2KUoMKxmlMILAlkEeAVcMc844xZqH3dedx+/4d2B2QyJTyWp6XSnpCNM8V68JTKnidcgnEGLMe8KmTvYjkApcCD/g1KKWOY7cJl87L4s2Pamnu7LE6HKUG5OjoCboGdLC+DWSFiBSLyDoRmdNv+++A7wHD9q8UkRtEpEhEimprawMWqApfly3Mptvp5uWdVVaHotSATrkSyAhsBSYbYxYA9wJrAUTkk0CNMWbLSC5ijLnfGFNojClMT08PXLQqbC2amMzElFjtjaWCliaQ4xhjmo0xrd73LwGRIpIGnAFcJiIHgCeB80TkMaviVOFPRPjU/Gze319PXWuX1eEodQxjDI72nqCbSBEsTCAiMkFExPt+qTeWemPMD4wxucaYKcBVwBvGmGusilOdGi5bmI3LbXhpR6XVoSh1jM4eN90ud9CtRgiB7cb7BLABmCEi5SLyNRG5UURu9B5yJbBTRIqBe4CrjDE6NaqyxMwJiUzPjOe57VqNpYJLsE5jAhCwFdqNMVcPs38NsGaYY94C3vJfVEoN7rIF2fz6lY+paOogJznW6nCUAoI7gVjdC0upoPGpBdkAPK+N6SqINLV3A5pAlApqk1PHsWBiMs9qNZYKIloCUSpErF6Qze7KZvZWt1gdilJA8M7EC5pAlDrGJxdkYRO0FKKChiNI10MHTSBKHSMjIYYzCtJ4trgC7RSogkFzRw8ikBAdsD5PPtMEotRxVi/M4XBDB1sPNVkdilI0dfSQGBOJLcimcgdNIEqd4BNzMomOsPHsdp2hV1kvWKcxAU0gSp0gISaSC2Zl8kKJrpeurBesM/GCJhClBrR6YTYNbd28u0/XS1fW0hKIUiHmnBme9dKf3abVWMpawTqRImgCUWpA0RF2Lp2fxSu7qmnvdlodjjqFaQlEqRC0emEO7d0uXt1VbXUo6hRljPG0gWgCUSq0LJ2SQlZSjA4qVJZp73bhdBstgSgVamw24bIF2az/uJaGtm6rw1GnoGCeBws0gSg1pNULc3C6DS/qQlPKAk3tmkCUClmzshKYlhGvvbGUJbQEolQIExEuX5RD0cFGDje0n7DfGKNzZqmA6UsgQTqQMPhm51IqyFy2IJtfvfwR9729n7k5SRyob+NgXbvn3/p2lk5N4ZGvLEEk+OYqUqGtOchLIJpAlBrGxJQ4lkwZz+ObDgEQZbcxMSWWKanjmJgSx6u7qnllVzWfmDPB4khVuGnqCN7VCEETiFIjsuYLp7G3upXJqXFkJ8di986M6nS5+cTv1nPXuj2cNzODSLvWCiv/cXT0YLcJ8UE4lTtoG4hSI5KZGMOZ09KYmBLXlzwAIuw2fnjJLMrq2nh840ELI1ThqHcUerBWj2oCUWqUzpuZwen5qfz+9b19jZ5K+YOjwxm01VcwwgQiIvkiEu19v1JEviUiyYENTanQICLcceksmjp6+L8391kdjgojjo7gnUgRRl4C+QfgEpEC4EFgKvDXgEWlVIiZk53Epxfl8vB7Bwbs7quULxzt3aFfAgHcxhgncAXwO2PMrUDWUCeIyEMiUiMiOwfZv1JEHCKy3fv6kXf7RBF5U0R2i8iHIvLvJ/MNKWWV2z4xHZsNfvnyR1aHosJEMM/ECyNPID0icjXwb8AL3m3DfVePAKuGOeYdY8xC7+tO7zYn8F1jzCxgOXCziMweYZxKWSYrKZbrz8rj+eIjbDvUaHU4KgwE80y8MPIE8hVgBfAzY0yZiEwFHhvqBGPMeqDhZAMyxlQaY7Z637cAu4Gck72OUlb4+jn5pMVH87MXd+sIdYscrG/jM394P+SrEt1uEx4lEGPMLmPMt4wxT4jIeCDBGHOXH+6/QkSKRWSdiMw5fqeITAEWAZv8cC+lAi4+OoLvXjSdooONPPXBYbqduqb6WPvntgq2HGzk3jf2Wh3KqLR2O3Gb4B1ECCPvhfWWiCSKSApQDDwsIr8d5b23ApONMQuAe4G1x90zHk/j/beNMc1DxHaDiBSJSFFtbe0oQ1Jq9D67OJdZWYnc/swO5v3kZT533wbuWreHV3dVU9/aZXV4Ye+NPTUAPLO1gvLG0C2FOIJ8Jl4YeRVWkvdD/NPAw8aYxcAFo7mxMabZGNPqff8SECkiaQAiEokneTxujHlmmOvcb4wpNMYUpqenjyYkpfwiwm7jqa8v5/++eBrXLJ9Mj9vNg++Wcv2fi1j836/xsxd3WR1i2Kpp7qSk3ME1yychAn98u9TqkHwW7BMpwsinMokQkSzgc8Ad/rixiEwAqo0xRkSW4klm9eIZcvkgsNsYM9pSjlKWSIyJ5JJ5WVwyz9NZsbPHxc4KBz99YRdvflTLHZdaHGCY6i19XLN8Mi43PFV0mG+eV0BmYozFkZ28YJ9IEUZeArkTeBnYb4z5QETygCErGEXkCWADMENEykXkayJyo4jc6D3kSmCniBQD9wBXGU+r4xnAl4Dz+nXxvcSH702poBETaadwSgpnFKRxoK5N20YC5PU9NeQkxzIjM4FvnJOPy224f31olkKCfS0QGGEJxBjzd+Dv/b4uBT4zzDlXD7N/DbBmgO3vAsE58YtSozQ9MwGn23Cgvo3pmQlWhxNWOntcvLu3js8W5iIiTEqNY/XCbB7fdJBvrPT0jgslTSGQQEbaiJ4rIv/0DgysFpF/iEhuoINTKtxMy4wH4OPqFosjCT8bSuvp6HFx3syMvm03n1tAl9PNg++WWRiZb0KhBDLSKqyHgeeAbDxjMp73blNKnYT89HhsAh9Xt1odSth5Y3cNcVF2luel9m3LT4/n0nlZ/Pn9AzS1d1sY3clzdPQQaRfiouxWhzKokSaQdGPMw8YYp/f1CKBdnpQ6STGRdialxLGvRksg/mSM4fXd1ZxZkEZM5LEfuDefW0Bbt4uH3ztgTXA+Cvap3GHkCaRORK4REbv3dQ1QH8jAlApX0zITwq4E0trlxO22buT9nqoWjjg6OX9Wxgn7ZmUlcuHsTB5+r4yWztCZbt/RHtwz8cLIE8hX8XThrQIq8fSg+kqgglIqnE3LiA+rnlgH69tY8t+vcdYv3+RXL+9hrwXtO6/vrgbg3JknJhCAW84roLnTyV9CaNGvYJ/GBEY+lckhY8xlxph0Y0yGMeZyPIMKlVInqX9PrHDwu9f2YjDkZ8Tzh7f2c+Hd67n0nnf40/pSqps7xySG1/fUsCA3iYyEgcd7zM9N5pzp6TzwThnt3c4xiWm0wiaBDOI7fotCqVNIQUb49MTaU9XM2u0VXHv6VP781aVs/OH5/OiTs4mwCT97aTfLf/E6n7h7PT94poS/FR1mX02LT1VdQ5XW6lq72H64ifNnZQ55jW+dP42Gtm4eCpEeWcE+Ey+MfCT6QIK3ZUepIFaQ4emJtTcM2kF+88rHxEdHcOM5eQBkJMTw1TOn8tUzp1Ja28qLJZUUHWzkxZJKnth8GPB0Sz1tUjJ3rp7LxJS4Ye+xt7qFT615l29fMJ0bz8k/Yf+be2owhmO67w5k8eTxXDg7k/veLuXqpZNIDfJxIaFQAhlNAtG5qpXyQW9PrL0h3hNr66FGXt1VzW0XTSc5LuqE/Xnp8dxy/jTAMzV5aV0rWw82sfVQI08VHWbttoq+/UN586MaOnvc3LVuD7GRdv7t9CnH7H9jTw0TEmOYk5047LW+v2oGF929nnvf2MdPLjthAvCg4XYbmjuDP4EMWYUlIi0i0jzAqwXPmBCllA8KMhJCvgTy65c/Ii0+iq+cMXXYY202oSAjgc8tmchdn5nPzAmJbCgdWUfOTaUNTEmN48LZmfz4uQ/5W9Hhvn1dThfrP67lvFkZI+ruWpCRwOeXTOTxTQc5VB+8M/W2dDoxhtDuhWWMSTDGJA7wSjDGjKb0otQpbXpmPGUh3BPrvX11vL+/npvPLWBc9Ml/FKzIS2XLwUa6nK4hj3O5DZvLGliRn8aaLyzirGlp3P6PEp4vPgLA5rIG2rpdnD9M9VV/375gOnab8KtXgnfp4d5R6AOV7ILJaBrRlVI+mpYZj9NtOBiCPbGMMfzy5Y/ISY7lC8sm+XSNFfmpdDndbDvUNORxu44009LlZHleCtERdu7/UiGFk1O49antvLqrmtd31xATaeOMgrQR3zszMYbrzvQsPVxSPvT9rRIK05iAJhClLDEtwzORYigOKHxlVzXFh5v49wumER3h2zQbS6emYBPYsH/oaqxNZZ79y6Z6pieJjbLz4LWFzMlJ4ubHt/Ls9grOyD9x9Plwvn5OHinjorhr3Z6gXHq4qcMz7YomEKXUCQoy4hEJva68Lrfh1y9/RH76OD69KMfn6yTFRjInO2nYdpCNpfVMTRvHhKSj4zsSYiJ59CtLyEsfR2N7z7DddweSEBPJLecV8P7+et7+OPhWMtUSiFJqUEfnxAqtEsiz2yvYW9PKdy+aQYR9dB8fK/JT2X6oic6egdtBXG7DprIGlk1NOWFfclwUj123jO9eOJ3VC33rz/OFZZOYmBLLXev24LJwGpaBaAJRSg1pWkZCSJVAXG7D3a99zNycRFbNmTDq663IS6Xb5WbrwcYB9++ubKal03nM7Lr9pcVHc8v503xqxAeIjrBz20Uz2FPVwtptFT5dI1CONqJrAlFKDaC3J1aPKzR6YlU6Ojjc0MHVSydhs41+HHHhlPHYbTJoNdZG7/ZleSeWQPzlU/OzmZeTxG9f/XjYHmFjydHRQ1SE7aTbdsaaJhClLNLbE+tAXWj0xCpv7ABg0ghGj49EQkwkc3OSBm1I3+gd/5GVFOuX+w3EZhNuOa+AiqYOig4MXBI6ntttqAnwHF+O9uAfRAiaQJSyTKj1xOpNILnj/ZNAwFONVVzedMIEh2634YMDDX29rwJpRX4qNoFNZQ0jOv7pLeWc+T9vUtMSuCQSCtOYgCYQpSyTn+7piRUqU5pUeBNIdvLAM976YkV+Kj0uc8Jf/7urmnF09LA8P3DVV70SYjw9wjaXjWxk/Bt7auh2DT+GZTRCYSJF0ASilGVio7xzYoVMCaSdzMRon8d+DKRw8ngiBmgH2VjqKQ2MRQkEPONSth1qGrYdxO02bPQmmkAOQtQSiFJqWNMyEkKmBFLe2OHX6iuAcdERLJiYfEI7yKbSeialxJGdHLj2j/6WTk2hy+lmR7ljyON2VzXT1O7pIVUyzLGj0aRtIEqp4UwLoZ5Y5U3t5I73/wf6irxUdlQ4aO3ytIO4veM/lgew99Xxlkzx3Gu4dpDeRHfujHRKyh0BG8Xe3BH8y9mCJhClLDU9M54eV/D3xHK63FQ2dZITgBLBivxUXG7DB94P7z1VLZ72j0HGfwRCyrgopmfGs3kECWRKahyfmDMBR0cPBwMwo6/LbWjpcmoJRCk1tN6eWHuDfER6dUsXTrfxexUWeBZ6irLb+tpB+ua/GsMEAp5qrC0HG3EOUhp0utx9MwPPz00GoDgA7SDNITKIEAKYQETkIRGpEZGdg+xfKSIOEdnuff2o375VIvKRiOwTkdsDFaNSVuvtiRXsI9LLGzx/aQeiCism0s7CScl9Awc3ltYzMSU2IKWdoSydmkprl5PdlQP/X+z0zgy8Ij+VaZnxREfYAtIOEirTmEBgSyCPAKuGOeYdY8xC7+tOABGxA/8LXAzMBq4WkdkBjFMpy/T1xAryEkhFU+8YkMB8qK/IS2VnhQNHe4+n/WOMel/1t7SvHWTg7ry97R8r8lKJtNuYk50YkJ5YTZpAwBizHhjZyJxjLQX2GWNKjTHdwJPAar8Gp1QQmZYRz95gL4H0jQEJUALJT8Vt4LFNB2lq7xnz6iuACUkxTE6NG7Qd5P39dUzPjCc9wbOW+vzcZHZWNA9a5eUrLYGM3AoRKRaRdSLSu0BxDnC43zHl3m0DEpEbRKRIRIpqa4NvWmalhrR/Pzf9/bf84/aLMTYbJCbCTTfB/v1WR3aM8sZ2MhKiAzY30xZsOUoAABcgSURBVMKJyURF2PjTO6UAA87AOxaWTknhgwMNJ/Su6na6KTrQyIp+iW3BxCQ6elzsq/Vv6TFUJlIEaxPIVmCyMWYBcC+w1rt9oFnaBu0rZ4y53xhTaIwpTE9PD0CYSgXIunUwfz4LX36ahO4OxBhoaYEHHoD58z37g4RnDEjg2iRiIu0snjSepvYecsfHMtFP822drCVTU2hs7zlhmv3i8iY6elysyD+68mFvQ3rJYf+2g/QmEO3GOwRjTLMxptX7/iUgUkTS8JQ4JvY7NBc4YkGISgXO/v1w5ZXQ3o7Neew8UPT0QHu7Z3+QlETKGzvICUAPrP5W5Hv+uh+r0ecD6S35HD8e5P199YhwzNiUqanjSIiO8HtPLEd7aKxGCBYmEBGZICLifb/UG0s98AEwTUSmikgUcBXwnFVxKhUQv/mNJ1EMpacH7r57bOIZgsttONIU2BII0Leu+en51iWQSSlxZCZGn9AOsqG0jtlZiSTHRfVts9mEeblJfu+JVdfaTXx0hF+njAmUQHbjfQLYAMwQkXIR+ZqI3CgiN3oPuRLYKSLFwD3AVcbDCXwTeBnYDfzNGPNhoOJUyhKPPTayBPKXv4xNPEOobu70jgEJbAJZPHk8f71+GZePYqnc0RIRlk5NZXPZ0XaQzh4XWw82HdP+0Wt+bjJ7qpr9upZIWV0bk1OtqcI7Wb4t5TUCxpirh9m/BlgzyL6XgJcCEZdSQaF1hA2vIz0ugI524Q38h9rp/doYrLJ0agrPFx/hcEMHk1Lj2HqwkW6Xm9MLTkwgC3KT6HEZdle2sHBisl/uf6C+jXk5SX65VqBZ3QtLqVNTfLx/jwug8sbADSIMRkfbQTzjPt7fX4/dJn3zZfU335s0/DUepNvp5nBDO1PTxvnleoGmCUQpK1xzDUQO00gaGQlf+tLYxDOE8gZPCWSsR4ZbpSA9nvFxkX3tIBtK65mXk0RCzIn/X9lJMaTFR1Hsp55YhxvbcRuYkqoJRCk1mO9+d2QJ5NZbxyaeIZQ3dpAWH7gxIMHG5i1tbD7QQFuXk+LDTYM27IsI83OT/VYCKav1TKo5NV0TiFJqMPn58PTTEBd3QiLpttkxcXGe/fn5FgV4VKCmcQ9mS6emcLC+nRdKjuB0m74uxgOZn5vEvtrWvunoR+NAvTeBaAlEKTWkiy+GkhK44QbPCHSbDVdCAk8uXMUDa9Z69geBQA8iDEa9Y1HWvLmPSLtQOHnwkfELcpMxBnZWjL4aq6yujeS4SMaPixr+4CCgCUQpK+Xnw5o14HCAy4W9uZkNt/4X9xx009I5TDdfPzjc0E5nz+BdUN19Y0BCo1upv8zKSmBclJ3DDR0smjie2KjBq+/m53p6TPmjGqusri1k2j9AE4hSQecbK/Np6XTy+KZDAb2Py2245PfvcO8bewc9pqalix5X4MeABJsIu43F3l5XQ1VfAaTGR5OTHEuxHwYUHqhrIy9EemCBJhClgs783GTOLEjjwXfLhiwdjFZdaxctXU7e/njwSUhPtS68/fV25x0ugYBnYsXRlkA6ul0ccXQyRROIUmo0blqZT21LF09vKQ/YPSodnQB8eKQZR/vA1WW907ifalVYAFctmcj3Vs0YcPzH8ebnJnO4oYOGtm6f73ewwduArglEKTUaK/JTWTAxmT+u3+/39SZ6VXkTiDGDL6LUWwI5VcaA9JcaH81NKwuw2waaIPxY/mgH6evCqwlEKTUaIsJNK/M53NDBs9sDMxl1lcNTurDbpG898uN5xoBEDdmIrGBeThIijGpAYZm3C69WYSmlRu3CWZnMyU7k9mdKeOS9shMWORqtquYuIu3Csqkpfcu1Hm8spnEPBwkxkeSljRt1CSQ9IZr46IBNUeh3mkCUClI2m/D4dcs4e1o6P3l+F7c8sc0vg9V6VTk6yEyM4fT8VPZUtQxYf1/eeOoNIvTVgtxkissdPif6A/VtITOAsJcmEKWCWHJcFH/6ciHfWzWDl3ZUctmad/nYT+unVzo6mZAY09fLaNNx1VieMSCdmkBGaNHk8dS1drHf25ZxssrqQmcSxV6aQJQKcjabcNPKAh6/bjnNHU5Wr3mPf24bfe+s6uZOJiTFMD83mbgo+wntILWtXXS73KdkDyxfXDgrE4B1OypP+tyWzh7qWrtCqv0DNIEoFTJW5Kfy0rfOZF5OErc+VczD75X5fC1jDJWOTrKSYoi021gy5cR2kFN5DIgvJiTFUDh5PC/6kEAO1HmetZZAlFIBk5EYw1+vX8bCicn8vcj3Uoijo4cup5vMxBjAk5z21rRS09LZd0zfGJBTsAuvry6Zl8Weqhb2157cQmCldZ7jNYEopQIqwm7jglkZ7Kpspq61y6dr9A4izEryJIfe5Vo3lh5dC7w3geRoCWTELp43AYCXSk6uFNJbAgmVpWx7aQJRKgSdOS0dgPf21fl0flWzJ4FMSIoGYE52IgnREcdUY5U3tpM6Loq4qNDpVmq1rKRYFvtQjVVW10pOcmzIrbmiCUSpEDQvJ4mk2Eje3etjAnH0JhBP6SLCbmPp1BQ2lvZPIKfeNO7+cKm3Gqv0JKqxyurbmZIWWqUP0ASiVEiy24TT81N5d1+dT+MOqhydiEBGQnTfthX5qZTVtfUll4rGU28ad3/oq8YaYSnEGENZbWvItX+AJhClQtaZ09KodHT6NO6gytFJWnw0kfajHwHLve0gG0rrcLsN5U1aAvFFbzXWCyNsB2ls76G50xlS64D00gSiVIg6q8DTDvLu3sGnYx9MVbOnC29/s7MSSYqNZMP+eupau+h2urUB3UeXnEQ1Vlmd5w+AvBBZB70/TSBKhahJqXFMSonjXR8a0qscnX1deHvZbN55sUrrOdw3jbsmEF9cchLVWL0JREsgxxGRh0SkRkR2DnPcEhFxiciV/bb9UkQ+FJHdInKPiAw/p7JSp5gzp6WxsbSBnpOc8r3S0XFCCQTg9PxUDjd09E3vrm0gvslKiuW0Scm8uKNq2GMP1LVhtwkTU0LvWQe6BPIIsGqoA0TEDvwP8HK/bacDZwDzgbnAEuCcgEWpVIg6qyCN1i4n2w+PfBbY9m4nzZ3OE0ogACvy0wB42jtI8VRcB8RfLp2fze7K5r4SxmDK6tqYOD72mPaoUBHQiI0x64GGYQ67BfgHUNP/VCAGiAKigUigOhAxKhXKTs9Pwybwzkl0563qG0R4YgKZnhlP6rgoSuvaSBkXxbgQmlo82Iy0Gqusri3k5sDqZWnKE5Ec4Argvv7bjTEbgDeBSu/rZWPM7kGucYOIFIlIUW3tyTcmKhXKkuIimZebfFIN6X2DCAcogYhIX28sbf8Ynb5qrCF6YxljPNO4awLxye+A7xtjXP03ikgBMAvIBXKA80Tk7IEuYIy53xhTaIwpTE9PD3jASgWbs6elUVzuoLlz4HXNj3d0EOGJCQRgeb4mEH+5ZF4Wu4aoxqpp6aK926UJxEeFwJMicgC4Evg/EbkcT6lkozGm1RjTCqwDllsXplLB68yCNFxuM+iqgsc7Oo3JwAmkd14sbf8YvUvmZQGDV2P1JhZNID4wxkw1xkwxxkwBngZuMsasBQ4B54hIhIhE4mlAH7AKS6lT3aJJ44mLso94WpMqRyeJMRGDznGVnz6OW84r4IpFuf4M85SUneypxnqhpHLAGQNCuQsvBL4b7xPABmCGiJSLyNdE5EYRuXGYU58G9gM7gGKg2BjzfCBjVSpURUXYWJ6XOuLxIFWOzr5ZeAciInz3ohnMzk70V4intCtOy2V3ZTN/eqf0hH0H6tqIirCRHaKlvYB2sTDGXH0Sx17b770L+HogYlIqHJ1ZkMYbe2q8a5gPPZ6gqrmTzEGqr5T/fXHpJDaW1vPzl/aQnRzLJ+dn9+0rrWtjckocdltoDnOzug1EKeUHZ03zjN8YSTVWpaOTrAF6YKnAsNmE33x2AUumjOc7TxWzuezoyIYDIdyFFzSBKBUWCjLiyUyM5p1hqrF6XG7qWru0BDLGYiLt/OnLheSmxHL9n4vYV9OKy204WN9OniYQpZSVRIQzC9J5f59nJt3B1LR0YczAgwhVYCXHRfHoV5YSaReufXgzJeVNdLvcWgJRSlnvrGlpNLb38OGR5kGP6RsDolVYlpiYEsdD1y6hvrWbrz7yARC6XXhBE4hSYeOMAk87yDv7Bh+VPtwgQhV483OTuffqRTg6PAM/NYEopSyXnhDNzAkJQw4o7B1EqFVY1rpgdiZ3fWY+58/MOGZVyFCjM6UpFUaW56Xy1AeH6XG5B5zdtcrRQXSEjaTYSAuiU/19rnAinyucaHUYo6IlEKXCyLKpKXT0uCgpdwy4v6q5i6ykGHR5HeUPmkCUCiNLp6YAsLF04GqsKkfHgOuAKOULTSBKhZHU+GhmZCawqWzgZXgqHSeuha6UrzSBKBVmluelUHTgxGVu3W5DTbMOIlT+owlEqTCzPC+V9m4XOyqObQdpaO+m2+XWaUyU32gCUSrMDNYOcnQMSGjO/KqCjyYQpcJMbzvIxtJj20F0EKHyN00gSoWhgdpBdBCh8jdNIEqFoYHaQaocndhtQlp86I58VsFFE4hSYWigdpCq5k4yEqJDdvEiFXw0gSgVhlLjo5meGX9MO0iVo1MHESq/0gSiVJhanpd6TDtIpaND2z+UX2kCUSpMHd8OUt3cpT2wlF9pAlEqTPVvB2np7KG1y6kLSSm/0gSiVJhK69cOUt2sY0CU/2kCUSqM9baDHG7sAHQpW+VfmkCUCmO97SCv7qoGIEunMVF+FLAEIiIPiUiNiOwc5rglIuISkSv7bZskIq+IyG4R2SUiUwIVp1LhrLcd5MWSSgAyEnUQofKfQJZAHgFWDXWAiNiB/wFePm7Xn4FfGWNmAUuBmkAEqFS4620HcXT0kDIuiphIu9UhqTASsARijFkPDLyqzVG3AP+gX4IQkdlAhDHmVe91Wo0x7YGKU6lwt2xqKoAOIlR+Z1kbiIjkAFcA9x23azrQJCLPiMg2EfmVt6Qy2HVuEJEiESmqra0NZMhKhaTleZ4EooMIlb9Z2Yj+O+D7xhjXcdsjgLOA24AlQB5w7WAXMcbcb4wpNMYUpqenBypWpULWsjxPO4iWQJS/RVh470LgSREBSAMuEREnUA5sM8aUAojIWmA58KBVgSoVytLio7njkll9DepK+YtlCcQYM7X3vYg8ArxgjFnrra4aLyLpxpha4DygyKIwlQoL15+dZ3UIKgwFLIGIyBPASiBNRMqBHwORAMaY49s9+hhjXCJyG/C6eIonW4A/BSpOpZRSvglYAjHGXH0Sx1573NevAvP9HZNSSin/0ZHoSimlfKIJRCmllE80gSillPKJJhCllFI+0QSilFLKJ5pAlFJK+USMMVbH4Dci4gD2DrI7CXCMcPvx24b7Og2oO6lgR2awmP1xzlDHBepZBeo5DRabP84J1HM6flsw/0yN9DwrfqYguJ7VaJ/TUPsD9fs32Rjj2zxQxpiweQH3n+y+gbYfv20EXxeN9fcz2nOseFaBek6BfFaBek4DPJug/Zmy+lmF2+/fcMeE0u9fuFVhPe/DvoG2H79tuK8DxZf7jPQcfVYjOydQz+n4bcH8nEZ6nv5Mjf45DbU/6J5VWFVhWUVEiowxhVbHEez0OY2cPquR02c1MoF4TuFWArHK/VYHECL0OY2cPquR02c1Mn5/TloCUUop5RMtgSillPKJJhCllFI+0QTSj4g8JCI1IrLTh3MXi8gOEdknIvd41zLp3XeLiHwkIh+KyC/9G7U1AvGsROQnIlIhItu9r0v8H/nYC9TPlXf/bSJiRCTNfxFbI0A/Uz8VkRLvz9MrIpLt/8jHXoCe1a9EZI/3ef1TRJKHu5YmkGM9Aqzy8dw/ADcA07yvVQAici6wGphvjJkD/Hr0YQaFR/Dzs/K62xiz0Pt6aXQhBo1HCMCzEpGJwIXAoVHGFywewf/P6VfGmPnGmIXAC8CPRhtkkHgE/z+rV4G5xpj5wMfAD4a7kCaQfowx64GG/ttEJF9E/iUiW0TkHRGZefx5IpIFJBpjNhhPr4Q/A5d7d38DuMsY0+W9R01gv4uxEaBnFZYC+KzuBr4HhEVPmEA8J2NMc79Dx6HPaqhn9Yoxxuk9dCOQO1wcmkCGdz9wizFmMXAb8H8DHJMDlPf7uty7DWA6cJaIbBKRt0VkSUCjtdZonxXAN71F6IdEZHzgQrXcqJ6ViFwGVBhjigMdqMVG/TMlIj8TkcPAFwmfEshA/PH71+urwLrhbhiwJW3DgYjEA6cDf+9X9Rw90KEDbOv9SycCGA8sB5YAfxORPBNm/af99Kz+APzU+/VPgd/g+UEOK6N9ViISB9wBXBSYCIODn36mMMbcAdwhIj8Avgn82M+hWs5fz8p7rTsAJ/D4cPfVBDI0G9DkrT/tIyJ2YIv3y+fwfPD1L+7lAke878uBZ7wJY7OIuPFMalYbyMAtMOpnZYyp7nfen/DUWYej0T6rfGAqUOz9sMgFtorIUmNMVYBjH0v++P3r76/Ai4RhAsFPz0pE/g34JHD+iP7I9ffkWqH+AqYAO/t9/T7wWe97ARYMct4HeEoZgqfod4l3+43And7304HDeAdwhvorAM8qq98xtwJPWv09BuuzOu6YA0Ca1d9jMD4nYFq/Y24Bnrb6ewziZ7UK2AWkjzgGqx9CML2AJ4BKoAdPyeFreP7S+xdQ7H24Pxrk3EJgJ7AfWNObJIAo4DHvvq3AeVZ/n0H8rP4C7ABK8Py1lDVW30+oPavjjgmLBBKgn6l/eLeX4JlYMMfq7zOIn9U+PH/gbve+7hsuDp3KRCmllE+0F5ZSSimfaAJRSinlE00gSimlfKIJRCmllE80gSillPKJJhAV1kSkdYzv94CIzPbTtVzeWWR3isjzw82OKiLJInKTP+6t1EhoN14V1kSk1RgT78frRZijE84FVP/YReRR4GNjzM+GOH4K8IIxZu5YxKeUlkDUKUdE0kXkHyLygfd1hnf7UhF5X0S2ef+d4d1+rYj8XUSeB14RkZUi8paIPO1dP+HxfmsqvCUihd73rd6J/IpFZKOIZHq353u//kBE7hxhKWkDRydSjBeR10Vkq3ddh9XeY+4C8r2lll95j/0P731KROS//PgYldIEok5Jv8ez7sgS4DPAA97te4CzjTGL8Mza+vN+56wA/s0Yc57360XAt4HZQB5wxgD3GQdsNMYsANYD1/e7/++99x9ozqZjeOczOh/P6HyATuAKY8xpwLnAb7wJ7HZgv/GspfIfInIRnvUelgILgcUicvZw91NqpHQyRXUqugCY3W/W0kQRSQCSgEdFZBqeGUoj+53zqjGm//oLm40x5QAish3PvETvHnefbo5OCLkFz+JP4ElGvet6/JXBFxmL7XftLXgW/AHPHEY/9yYDN56SSeYA51/kfW3zfh2PJ6GsH+R+Sp0UTSDqVGQDVhhjOvpvFJF7gTeNMVd42xPe6re77bhrdPV772Lg36Uec7SRcbBjhtJhjFkoIkl4EtHNwD141rVIBxYbY3pE5AAQM8D5AvzCGPPHk7yvUiOiVVjqVPQKnnUhABCR3imwk4AK7/trA3j/jXiqzgCuGu5gY4wD+BZwm4hE4omzxps8zgUmew9tARL6nfoy8FXvWhGISI6IZPjpe1BKE4gKe3EiUt7v9R08H8aF3oblXXim3Af4JfALEXkPsAcwpm8D3xGRzUAW4BjuBGPMNjyzrF6FZ6GfQhEpwlMa2eM9ph54z9vt91fGmFfwVJFtEJEdwNMcm2CUGhXtxqvUGPOuKNhhjDEichVwtTFm9XDnKRVstA1EqbG3GFjj7TnVRBgu26tODVoCUUop5RNtA1FKKeUTTSBKKaV8oglEKaWUTzSBKKWU8okmEKWUUj75/8xxTGWYd5ABAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.recorder.plot(skip_end=10, suggestion=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    \n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(1, max_lr=8e-5, moms=(0.8, 0.7))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "learner.save('first_cycle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed_all(seed)\n",
    "learner.load('first_cycle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.freeze_to(-2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = 1e-5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.save('second_cycle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed_all(seed)\n",
    "learner.load('second_cycle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.freeze_to(-3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.save('third_cycle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed_all(seed)\n",
    "learner.load('third_cycle')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.unfreeze()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(2, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.predict('This is the best movie of 2020')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.predict('This is the worst movie of 2020')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.export(file = 'transformer.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "export_learner = load_learner(path, file = 'transformer.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "export_learner.predict('This is the worst movie of 2020')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    the get_preds method does not yield the elements in order by default\n",
    "    we borrow the code from the RNNLearner to resort the elements into their correct order\n",
    "    \"\"\"\n",
    "    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n",
    "    sampler = [i for i in databunch.dl(ds_type).sampler]\n",
    "    reverse_sampler = np.argsort(sampler)\n",
    "    return preds[reverse_sampler, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_preds = get_preds_as_nparray(DatasetType.Test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(path / 'sampleSubmission.csv')\n",
    "sample_submission['Sentiment'] = np.argmax(test_preds,axis=1)\n",
    "sample_submission.to_csv(\"predictions.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "# create a link to download the dataframe which was saved with .to_csv method\n",
    "create_download_link(filename='predictions.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}